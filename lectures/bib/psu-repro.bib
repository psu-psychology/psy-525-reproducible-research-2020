
@ARTICLE{Gorgolewski2016-fd,
  title    = "A Practical Guide for Improving Transparency and Reproducibility
              in Neuroimaging Research",
  author   = "Gorgolewski, Krzysztof J and Poldrack, Russell A",
  journal  = "PLoS biology",
  volume   =  14,
  number   =  7,
  pages    = "e1002506",
  month    =  jul,
  year     =  2016,
  url      = "http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002506",
  keywords = "cognitive neuroscience; Neuroimaging; Neuroscience; Open Science;
              Peer review; Reproducibility; Scientists; Statistical data",
  issn     = "1544-9173, 1545-7885",
  doi      = "10.1371/journal.pbio.1002506"
}

@ARTICLE{Camerer2018-tr,
  title     = "Evaluating the replicability of social science experiments in
               Nature and Science between 2010 and 2015",
  author    = "Camerer, Colin F and Dreber, Anna and Holzmeister, Felix and Ho,
               Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and
               Kirchler, Michael and Nave, Gideon and Nosek, Brian A and
               Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan,
               Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and
               Heikensten, Emma and Hummer, Lily and Imai, Taisuke and
               Isaksson, Siri and Manfredi, Dylan and Rose, Julia and
               Wagenmakers, Eric-Jan and Wu, Hang",
  journal   = "Nature Human Behaviour",
  publisher = "Nature Publishing Group",
  pages     = "1",
  month     =  aug,
  year      =  2018,
  url       = "https://www.nature.com/articles/s41562-018-0399-z",
  language  = "en",
  issn      = "2397-3374, 2397-3374",
  doi       = "10.1038/s41562-018-0399-z"
}

@article{sejnowski2014putting,
  title={Putting big data to good use in neuroscience},
  author={Sejnowski, Terrence J and Churchland, Patricia S and Movshon, J Anthony},
  journal={Nature neuroscience},
  doi={10.1038/nn.3839},
  volume={17},
  number={11},
  pages={1440--1441},
  year={2014},
  publisher={Nature Publishing Group}
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {1946-6234, 1946-6242},
	url = {http://stm.sciencemag.org/content/8/341/341ps12},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.},
	language = {en},
	number = {341},
	urldate = {2016-10-09},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	pmid = {27252173},
	pages = {341ps12--341ps12},
	file = {Goodman et al. - 2016 - What does research reproducibility mean.pdf:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/M2JX5V9V/Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/TNSTI5G9/341ps12.html:text/html}
}

@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Related ResourcesIn Science Magazine Podcasts Science Podcast: 28 August Show Science 28 August 2015: 999.In Depth Reproducibility Many psychology papers fail replication test John BohannonScience 28 August 2015: 910-911.Policy Forum Scientific Standards Promoting an open research culture B. A. Nosek et al.Science 26 June 2015: 1422-1425.Editorial EDITORIAL: Solving reproducibility Stuart BuckScience 26 June 2015: 1403.More related articles... View larger version: In this page In a new window Download PowerPoint Slide for Teaching Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716},
	language = {en},
	number = {6251},
	urldate = {2015-08-28},
	journal = {Science},
	author = {Collaboration, Open Science},
	month = aug,
	year = {2015},
	pages = {aac4716},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/UHASJEE8/Collaboration - 2015 - Estimating the reproducibility of psychological.pdf:application/pdf;Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/M2DUJAI7/Collaboration - 2015 - Estimating the reproducibility of psychological.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/SI9HSIGX/Collaboration - 2015 - Estimating the reproducibility of psychological.html:text/html;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/IN75VC96/aac4716.html:text/html}
}


@article{gilbert_comment_2016,
	title = {Comment on “{Estimating} the reproducibility of psychological science”},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6277/1037.2},
	doi = {10.1126/science.aad7243},
	abstract = {A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.},
	language = {en},
	number = {6277},
	urldate = {2016-03-27},
	journal = {Science},
	author = {Gilbert, Daniel T. and King, Gary and Pettigrew, Stephen and Wilson, Timothy D.},
	month = mar,
	year = {2016},
	pmid = {26941311},
	pages = {1037--1037},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/2IMDZZE6/Gilbert et al. - 2016 - Comment on “Estimating the reproducibility of psyc.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/H8PEE6R2/1037.html:text/html}
}


@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	language = {en},
	number = {6242},
	urldate = {2016-09-08},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pmid = {26113702},
	pages = {1422--1425},
	file = {Nosek et al. - 2015 - Promoting an open research culture.pdf:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/UZE4ESDI/Nosek et al. - 2015 - Promoting an open research culture.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/6CI232F3/1422.full.html:text/html;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/KICS687H/1422.html:text/html}
}

@article{lash2015declining,
  title={Declining the Transparency and Openness Promotion Guidelines},
  author={Lash, Timothy L},
  url={http://journals.lww.com/epidem/Fulltext/2015/11000/Declining_the_Transparency_and_Openness_Promotion.1.aspx},
  journal={Epidemiology},
  volume={26},
  number={6},
  pages={779--780},
  year={2015},
  publisher={LWW}
}


@article{baker_over_2015,
	title = {Over half of psychology studies fail reproducibility test},
	url = {http://www.nature.com/news/over-half-of-psychology-studies-fail-reproducibility-test-1.18248},
	doi = {10.1038/nature.2015.18248},
	abstract = {Largest replication study to date casts doubt on many published positive results.},
	urldate = {2017-01-02},
	journal = {Nature News},
	author = {Baker, Monya},
	month = aug,
	year = {2015},
	file = {Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/2JGJAIVI/over-half-of-psychology-studies-fail-reproducibility-test-1.html:text/html}
}

@article{miguel_promoting_2014,
	title = {Promoting {Transparency} in {Social} {Science} {Research}},
	volume = {343},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/content/343/6166/30},
	doi = {10.1126/science.1245317},
	language = {en},
	number = {6166},
	urldate = {2015-06-15},
	journal = {Science},
	author = {Miguel, E. and Camerer, C. and Casey, K. and Cohen, J. and Esterling, K. M. and Gerber, A. and Glennerster, R. and Green, D. P. and Humphreys, M. and Imbens, G. and Laitin, D. and Madon, T. and Nelson, L. and Nosek, B. A. and Petersen, M. and Sedlmayr, R. and Simmons, J. P. and Simonsohn, U. and Laan, M. Van der},
	month = jan,
	year = {2014},
	pmid = {24385620},
	pages = {30--31},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/RAZJHJKX/Miguel et al. - 2014 - Promoting Transparency in Social Science Research.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/N5T9BZP2/Miguel et al. - 2014 - Promoting Transparency in Social Science Research.html:text/html}
}


@article{hauser_retracted:_2002,
	title = {{RETRACTED}: {Rule} learning by cotton-top tamarins},
	volume = {86},
	issn = {0010-0277},
	shorttitle = {{RETRACTED}},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027702001397},
	doi = {10.1016/S0010-0277(02)00139-7},
	abstract = {This article has been retracted: please see Elsevier Policy on Article Withdrawal (http://www.elsevier.com/locate/withdrawalpolicy). This article has been retracted at the request of the authors. An internal investigation at Harvard University of the reported research found that the data do not support the reported findings. The authors are therefore retracting this article. M. Hauser accepts responsibility for the error.},
	number = {1},
	urldate = {2017-01-03},
	journal = {Cognition},
	author = {Hauser, Marc D. and Weiss, Daniel and Marcus, Gary},
	month = nov,
	year = {2002},
	pages = {B15--B22}
}


@article{saffran_grammatical_2008,
	title = {Grammatical pattern learning by human infants and cotton-top tamarin monkeys},
	volume = {107},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027707002697},
	doi = {10.1016/j.cognition.2007.10.010},
	abstract = {There is a surprising degree of overlapping structure evident across the languages of the world. One factor leading to cross-linguistic similarities may be constraints on human learning abilities. Linguistic structures that are easier for infants to learn should predominate in human languages. If correct, then (a) human infants should more readily acquire structures that are consistent with the form of natural language, whereas (b) non-human primates’ patterns of learning should be less tightly linked to the structure of human languages. Prior experiments have not directly compared laboratory-based learning of grammatical structures by human infants and non-human primates, especially under comparable testing conditions and with similar materials. Five experiments with 12-month-old human infants and adult cotton-top tamarin monkeys addressed these predictions, employing comparable methods (familiarization–discrimination) and materials. Infants rapidly acquired complex grammatical structures by using statistically predictive patterns, failing to learn structures that lacked such patterns. In contrast, the tamarins only exploited predictive patterns when learning relatively simple grammatical structures. Infant learning abilities may serve both to facilitate natural language acquisition and to impose constraints on the structure of human languages.},
	number = {2},
	urldate = {2017-01-03},
	journal = {Cognition},
	author = {Saffran, Jenny and Hauser, Marc and Seibel, Rebecca and Kapfhamer, Joshua and Tsao, Fritz and Cushman, Fiery},
	month = may,
	year = {2008},
	keywords = {Grammar learning, Infants, monkeys, Statistical learning},
	pages = {479--500}
}


@article{hauser_rhesus_2007,
	title = {Rhesus monkeys correctly read the goal-relevant gestures of a human agent},
	volume = {274},
	copyright = {© 2007 The Royal Society},
	issn = {0962-8452, 1471-2954},
	url = {http://rspb.royalsocietypublishing.org/content/274/1620/1913},
	doi = {10.1098/rspb.2007.0586},
	abstract = {When humans point, they reveal to others their underlying intent to communicate about some distant goal. A controversy has recently emerged based on a broad set of comparative and phylogenetically relevant data. In particular, whereas chimpanzees (Pan troglodytes) have difficulty in using human-generated communicative gestures and actions such as pointing and placing symbolic markers to find hidden rewards, domesticated dogs (Canis familiaris) and silver foxes (Urocyon cinereoargenteus) readily use such gestures and markers. These comparative data have led to the hypothesis that the capacity to infer communicative intent in dogs and foxes has evolved as a result of human domestication. Though this hypothesis has met with challenges, due in part to studies of non-domesticated, non-primate animals, there remains the fundamental question of why our closest living relatives, the chimpanzees, together with other non-human primates, generally fail to make inferences about a target goal of an agent's communicative intent. Here, we add an important wrinkle to this phylogenetic pattern by showing that free-ranging rhesus monkeys (Macaca mulatta) draw correct inferences about the goals of a human agent, using a suite of communicative gestures to locate previously concealed food. Though domestication and human enculturation may play a significant role in tuning up the capacity to infer intentions from communicative gestures, these factors are not necessary.},
	language = {en},
	number = {1620},
	urldate = {2017-01-03},
	journal = {Proceedings of the Royal Society of London B: Biological Sciences},
	author = {Hauser, Marc D. and Glynn, David and Wood, Justin},
	month = aug,
	year = {2007},
	pmid = {17540661},
	pages = {1913--1918},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/U7ZN48NB/Hauser et al. - 2007 - Rhesus monkeys correctly read the goal-relevant ge.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/TH9XRR2C/1913.html:text/html}
}


@article{wood_perception_2007,
	title = {The {Perception} of {Rational}, {Goal}-{Directed} {Action} in {Nonhuman} {Primates}},
	volume = {317},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/317/5843/1402},
	doi = {10.1126/science.1144663},
	abstract = {Humans are capable of making inferences about other individuals' intentions and goals by evaluating their actions in relation to the constraints imposed by the environment. This capacity enables humans to go beyond the surface appearance of behavior to draw inferences about an individual's mental states. Presently unclear is whether this capacity is uniquely human or is shared with other animals. We show that cotton-top tamarins, rhesus macaques, and chimpanzees all make spontaneous inferences about a human experimenter's goal by attending to the environmental constraints that guide rational action. These findings rule out simple associative accounts of action perception and show that our capacity to infer rational, goal-directed action likely arose at least as far back as the New World monkeys, some 40 million years ago.
Apes, as well as New and Old World monkeys, can analyze goal-directed actions and infer the underlying rationale.
Apes, as well as New and Old World monkeys, can analyze goal-directed actions and infer the underlying rationale.},
	language = {en},
	number = {5843},
	urldate = {2017-01-03},
	journal = {Science},
	author = {Wood, Justin N. and Glynn, David D. and Phillips, Brenda C. and Hauser, Marc D.},
	month = sep,
	year = {2007},
	pmid = {17823353},
	pages = {1402--1405},
	file = {Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/U679G9UK/1402.html:text/html}
}

@article{hadley_wickham_tidy_2014,
	title = {Tidy {Data}},
	volume = {59},
	url = {https://www.jstatsoft.org/article/view/v059i10},
	doi = {10.18637/jss.v059.i10},
	abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
	number = {10},
	urldate = {2016-11-19},
	journal = {Journal of Statistical Software},
	author = {Wickham, Hadley},
	year = {2014},
	file = {Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/XWSTDFGN/Tidy Data  Wickham  Journal of Statistical Softw.html:text/html}
}

@article{szucs_empirical_2016,
	title = {Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature},
	copyright = {© 2016, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {http://biorxiv.org/content/early/2016/08/25/071530},
	doi = {10.1101/071530},
	abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by extracting more than 100,000 statistical records from about 10,000 cognitive neuroscience and psychology papers published during the past 5 years. The reported median effect size was d=0.93 (inter-quartile range: 0.64-1.46) for nominally statistically significant results and d=0.24 (0.11-0.42) for non-significant results. Median power to detect small, medium and large effects was 0.12, 0.44 and 0.73, reflecting no improvement through the past half-century. Power was lowest for cognitive neuroscience journals. 14\% of papers reported some statistically significant results, although the respective F statistic and degrees of freedom proved that these were non-significant; p value errors positively correlated with journal impact factors. False report probability is likely to exceed 50\% for the whole literature. In light of our findings the recently reported low replication success in psychology is realistic and worse performance may be expected for cognitive neuroscience.},
	language = {en},
	urldate = {2016-10-02},
	journal = {bioRxiv},
	author = {Szucs, Denes and Ioannidis, John PA},
	month = aug,
	year = {2016},
	pages = {071530},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/VS2PGZCM/Szucs and Ioannidis - 2016 - Empirical assessment of published effect sizes and.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/HUEFQC2G/071530.html:text/html}
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {© 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Leading voices in the reproducibility landscape call for the adoption of measures to optimize key elements of the scientific process.},
	language = {en},
	urldate = {2017-01-10},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Sert, Nathalie Percie du and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
	file = {Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/XBPS9Q24/Munafò et al. - 2017 - A manifesto for reproducible science.html:text/html}
}

@article{gilmore_big_2016,
	title = {From big data to deep insight in developmental science},
	volume = {7},
	issn = {1939-5086},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wcs.1379/abstract},
	doi = {10.1002/wcs.1379},
	abstract = {The use of the term ‘big data’ has grown substantially over the past several decades and is now widespread. In this review, I ask what makes data ‘big’ and what implications the size, density, or complexity of datasets have for the science of human development. A survey of existing datasets illustrates how existing large, complex, multilevel, and multimeasure data can reveal the complexities of developmental processes. At the same time, significant technical, policy, ethics, transparency, cultural, and conceptual issues associated with the use of big data must be addressed. Most big developmental science data are currently hard to find and cumbersome to access, the field lacks a culture of data sharing, and there is no consensus about who owns or should control research data. But, these barriers are dissolving. Developmental researchers are finding new ways to collect, manage, store, share, and enable others to reuse data. This promises a future in which big data can lead to deeper insights about some of the most profound questions in behavioral science. WIREs Cogn Sci 2016, 7:112–126. doi: 10.1002/wcs.1379 For further resources related to this article, please visit the WIREs website.},
	language = {en},
	number = {2},
	urldate = {2016-09-28},
	journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
	author = {Gilmore, Rick O.},
	month = mar,
	year = {2016},
	pages = {112--126},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/4PUP58SB/Gilmore - 2016 - From big data to deep insight in developmental sci.pdf:application/pdf;Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/HS4XE5V5/Gilmore - 2016 - From big data to deep insight in developmental sci.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/DBEQWDQB/abstract.html:text/html;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/U8P6FEU6/Gilmore - 2016 - From big data to deep insight in developmental sci.html:text/html}
}

@article{maxwell_persistence_2004,
	title = {The {Persistence} of {Underpowered} {Studies} in {Psychological} {Research}: {Causes}, {Consequences}, and {Remedies}},
	volume = {9},
	copyright = {(c) 2012 APA, all rights reserved},
	issn = {1939-1463(Electronic);1082-989X(Print)},
	shorttitle = {The {Persistence} of {Underpowered} {Studies} in {Psychological} {Research}},
	doi = {10.1037/1082-989X.9.2.147},
	abstract = {Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The "curse of multiplicities" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing.},
	number = {2},
	journal = {Psychological Methods},
	author = {Maxwell, Scott E.},
	year = {2004},
	keywords = {*Effect Size (Statistical), *Hypothesis Testing, *Methodology, *Psychology, *Statistical Power, Confidence Limits (Statistics), Type I Errors},
	pages = {147--163}
}


@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	url = {http://dx.doi.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Published research findings are sometimes refuted by subsequent evidence, says Ioannidis, with ensuing confusion and disappointment.},
	number = {8},
	urldate = {2015-07-27},
	journal = {PLoS Med},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
	file = {PLoS Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/HIEJMJ4S/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf}
}

@article{henrich_weirdest_2010,
	title = {The weirdest people in the world?},
	volume = {33},
	issn = {1469-1825},
	doi = {10.1017/S0140525X0999152X},
	abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers - often implicitly - assume that either there is little variation across human populations, or that these "standard subjects" are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species - frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior - hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
	language = {eng},
	number = {2-3},
	journal = {The Behavioral and Brain Sciences},
	author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
	month = jun,
	year = {2010},
	pmid = {20550733},
	keywords = {Behavioral Sciences, Cognition, Cross-Cultural Comparison, decision making, Humans, Morals, Population Groups, Visual Perception},
	pages = {61--83; discussion 83--135}
}


@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	number = {7604},
	urldate = {2017-01-19},
	journal = {Nature News},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	pages = {452}
}


@article{prinz_believe_2011,
	title = {Believe it or not: how much can we rely on published data on potential drug targets?},
	volume = {10},
	copyright = {© 2011 Nature Publishing Group},
	issn = {1474-1776},
	shorttitle = {Believe it or not},
	url = {http://www.nature.com/nrd/journal/v10/n9/full/nrd3439-c1.html},
	doi = {10.1038/nrd3439-c1},
	language = {en},
	number = {9},
	urldate = {2017-01-19},
	journal = {Nature Reviews Drug Discovery},
	author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
	month = sep,
	year = {2011},
	pages = {712--712},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/27RAXMNU/Prinz et al. - 2011 - Believe it or not how much can we rely on publish.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/CKRHH4MW/nrd3439-c1.html:text/html}
}

@article{begley_drug_2012,
	title = {Drug development: {Raise} standards for preclinical cancer research},
	volume = {483},
	copyright = {© 2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	shorttitle = {Drug development},
	url = {http://www.nature.com/nature/journal/v483/n7391/full/483531a.html},
	doi = {10.1038/483531a},
	abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
	language = {en},
	number = {7391},
	urldate = {2017-01-19},
	journal = {Nature},
	author = {Begley, C. Glenn and Ellis, Lee M.},
	month = mar,
	year = {2012},
	keywords = {Cancer, Drug discovery, Publishing},
	pages = {531--533},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/8NNJIN7T/Begley and Ellis - 2012 - Drug development Raise standards for preclinical .pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/45QPPZ7P/483531a.html:text/html}
}


@article{fanelli_how_2009,
	title = {How {Many} {Scientists} {Fabricate} and {Falsify} {Research}? {A} {Systematic} {Review} and {Meta}-{Analysis} of {Survey} {Data}},
	volume = {4},
	issn = {1932-6203},
	shorttitle = {How {Many} {Scientists} {Fabricate} and {Falsify} {Research}?},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738},
	doi = {10.1371/journal.pone.0005738},
	abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, “cooking” of data, etc… Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86–4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once –a serious form of misconduct by any standard– and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91–19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words “falsification” or “fabrication”, and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
	number = {5},
	urldate = {2017-02-16},
	journal = {PLOS ONE},
	author = {Fanelli, Daniele},
	month = may,
	year = {2009},
	keywords = {deception, Health services research, meta-analysis, research integrity, Scientific misconduct, Scientists, Social research, Surveys},
	pages = {e5738},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/U87JM4RV/Fanelli - 2009 - How Many Scientists Fabricate and Falsify Research.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/5NUSU8QA/article.html:text/html}
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976},
	shorttitle = {False-{Positive} {Psychology}},
	url = {http://dx.doi.org/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2017-02-16},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pages = {1359--1366},
	file = {SAGE PDF Full Text:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/3RAGPSRE/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf}
}

@article{wicherts_poor_2006,
	title = {The poor availability of psychological research data for reanalysis},
	volume = {61},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {1935-990X 0003-066X},
	doi = {10.1037/0003-066X.61.7.726},
	abstract = {The origin of the present comment lies in a failed attempt to obtain, through e-mailed requests, data reported in 141 empirical articles recently published by the American Psychological Association (APA). Our original aim was to reanalyze these data sets to assess the robustness of the research findings to outliers. We never got that far. In June 2005, we contacted the corresponding author of every article that appeared in the last two 2004 issues of four major APA journals. Because their articles had been published in APA journals, we were certain that all of the authors had signed the APA Certification of Compliance With APA Ethical Principles, which includes the principle on sharing data for reanalysis. Unfortunately, 6 months later, after writing more than 400 e-mails--and sending some corresponding authors detailed descriptions of our study aims, approvals of our ethical committee, signed assurances not to share data with others, and even our full resumes-we ended up with a meager 38 positive reactions and the actual data sets from 64 studies (25.7\% of the total number of 249 data sets). This means that 73\% of the authors did not share their data.},
	language = {English},
	number = {7},
	journal = {American Psychologist},
	author = {Wicherts, Jelte M. and Borsboom, Denny and Kats, Judith and Molenaar, Dylan},
	year = {2006},
	keywords = {*Compliance, *Data Collection, *Experimentation, *Professional Ethics, *Scientific Communication, data sharing},
	pages = {726--728}
}

@article{vanpaemel_are_2015,
	title = {Are {We} {Wasting} a {Good} {Crisis}? {The} {Availability} of {Psychological} {Research} {Data} after the {Storm}},
	volume = {1},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2474-7394},
	shorttitle = {Are {We} {Wasting} a {Good} {Crisis}?},
	url = {http://www.collabra.org/articles/10.1525/collabra.13/},
	doi = {10.1525/collabra.13},
	abstract = {Article: Are We Wasting a Good Crisis? The Availability of Psychological Research Data after the Storm},
	language = {eng},
	number = {1},
	urldate = {2017-02-16},
	journal = {Collabra: Psychology},
	author = {Vanpaemel, Wolf and Vermorgen, Maarten and Deriemaecker, Leen and Storms, Gert},
	month = oct,
	year = {2015},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/TKWQNSXA/Vanpaemel et al. - 2015 - Are We Wasting a Good Crisis The Availability of .pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/FPXPS9VT/collabra.html:text/html}
}

@article{lacour_when_2014,
	title = {When contact changes minds: {An} experiment on transmission of support for gay equality},
	volume = {346},
	copyright = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	shorttitle = {When contact changes minds},
	url = {http://science.sciencemag.org/content/346/6215/1366},
	doi = {10.1126/science.1256151},
	abstract = {Can a single conversation change minds on divisive social issues, such as same-sex marriage? A randomized placebo-controlled trial assessed whether gay (n = 22) or straight (n = 19) messengers were effective at encouraging voters (n = 972) to support same-sex marriage and whether attitude change persisted and spread to others in voters’ social networks. The results, measured by an unrelated panel survey, show that both gay and straight canvassers produced large effects initially, but only gay canvassers’ effects persisted in 3-week, 6-week, and 9-month follow-ups. We also find strong evidence of within-household transmission of opinion change, but only in the wake of conversations with gay canvassers. Contact with gay canvassers further caused substantial change in the ratings of gay men and lesbians more generally. These large, persistent, and contagious effects were confirmed by a follow-up experiment. Contact with minorities coupled with discussion of issues pertinent to them is capable of producing a cascade of opinion change.
Dialogue opens the door to attitude change
Personal contact between in-group and out-group individuals of equivalent status can reduce perceived differences and thus improve intergroup relations. LaCour and Green demonstrate that simply a 20-minute conversation with a gay canvasser produced a large and sustained shift in attitudes toward same-sex marriage for Los Angeles County residents. Surveys showed persistent change up to 9 months after the initial conversation. Indeed, the magnitude of the shift for the person who answered the door was as large as the difference between attitudes in Georgia and Massachusetts.
Science, this issue p. 1366
Contact with minorities, coupled with discussion of issues pertinent to them, can produce lasting opinion change.
Contact with minorities, coupled with discussion of issues pertinent to them, can produce lasting opinion change.},
	language = {en},
	number = {6215},
	urldate = {2017-02-16},
	journal = {Science},
	author = {LaCour, Michael J. and Green, Donald P.},
	month = dec,
	year = {2014},
	pmid = {25504721},
	pages = {1366--1369},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/9DW3B83A/LaCour and Green - 2014 - When contact changes minds An experiment on trans.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/9JN6SU5B/1366.html:text/html}
}


@article{ferguson_everybody_2015,
	title = {“{Everybody} knows psychology is not a real science”: {Public} perceptions of psychology and how we can improve our relationship with policymakers, the scientific community, and the general public},
	volume = {70},
	copyright = {(c) 2016 APA, all rights reserved},
	issn = {1935-990X 0003-066X},
	shorttitle = {“{Everybody} knows psychology is not a real science”},
	doi = {10.1037/a0039405},
	abstract = {In a recent seminal article, Lilienfeld (2012) argued that psychological science is experiencing a public perception problem that has been caused by both public misconceptions about psychology, as well as the psychological science community’s failure to distinguish itself from pop psychology and questionable therapeutic practices. Lilienfeld’s analysis is an important and cogent synopsis of external problems that have limited psychological science’s penetration into public knowledge. The current article expands upon this by examining internal problems, or problems within psychological science that have potentially limited its impact with policymakers, other scientists, and the public. These problems range from the replication crisis and defensive reactions to it, overuse of politicized policy statements by professional advocacy groups such as the American Psychological Association (APA), and continued overreliance on mechanistic models of human behavior. It is concluded that considerable problems arise from psychological science’s tendency to overcommunicate mechanistic concepts based on weak and often unreplicated (or unreplicable) data that do not resonate with the everyday experiences of the general public or the rigor of other scholarly fields. It is argued that a way forward can be seen by, on one hand, improving the rigor and transparency of psychological science, and making theoretical innovations that better acknowledge the complexities of the human experience.},
	language = {English},
	number = {6},
	journal = {American Psychologist},
	author = {Ferguson, Christopher J.},
	year = {2015},
	keywords = {*Psychology, *Public Opinion, *Sciences, Policy Making},
	pages = {527--542}
}


@article{higginson_current_2016,
	title = {Current {Incentives} for {Scientists} {Lead} to {Underpowered} {Studies} with {Erroneous} {Conclusions}},
	volume = {14},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000995},
	doi = {10.1371/journal.pbio.2000995},
	abstract = {We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10\%–40\% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved.},
	number = {11},
	urldate = {2017-01-16},
	journal = {PLOS Biology},
	author = {Higginson, Andrew D. and Munafò, Marcus R.},
	month = nov,
	year = {2016},
	keywords = {Careers, Careers in research, Drug discovery, Ecosystems, Peer review, Research assessment, Research errors, Scientists},
	pages = {e2000995},
	file = {Full Text PDF:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/JZ8FN8F4/Higginson and Munafò - 2016 - Current Incentives for Scientists Lead to Underpow.pdf:application/pdf;Snapshot:/Users/rick/Library/Application Support/Zotero/Profiles/isqcn0ks.default/zotero/storage/9GJNF7R2/article.html:text/html}
}

@article{gilmore-adolph-video-2017,
  title={Video can make science more open, transparent, robust, and reproducible},
  author={R.O. Gilmore and K.E. Adolph},
  month={feb},
  year={2017},
  url={http://osf.io/3kvp7}
}

@misc{https://doi.org/10.17910/B7CC74,
  doi = {10.17910/B7CC74},
  url = {https://doi.org/10.17910/B7CC74},
  author = {Tamis-LeMonda, Catherine},
  publisher = {Databrary},
  title = {Language, cognitive, and socio-emotional skills from 9 months until their transition to first grade in U.S. children from African-American, Dominican, Mexican, and Chinese backgrounds},
  year = {2013}
}


@ARTICLE{Grinvald2004-bg,
  title       = "{VSDI}: a new era in functional imaging of cortical dynamics",
  author      = "Grinvald, Amiram and Hildesheim, Rina",
  affiliation = "Department of Neurobiology, The Weizmann Institute of Science, Rehovot, 76100 Israel. Amiram.Grinvald@weizmann.ac.il",
  journal     = "Nature Review Neuroscience",
  volume      =  5,
  number      =  11,
  pages       = "874--885",
  month       =  nov,
  year        =  2004,
  url         = "http://dx.doi.org/10.1038/nrn1536",
  language    = "en",
  issn        = "1471-003X",
  pmid        = "15496865",
  doi         = "10.1038/nrn1536"
}

@ARTICLE{Button2013-ox,
  title    = "Power failure: why small sample size undermines the reliability
              of neuroscience",
  author   = "Button, Katherine S and Ioannidis, John P A and Mokrysz, Claire
              and Nosek, Brian A and Flint, Jonathan and Robinson, Emma S J and
              Munaf{\`o}, Marcus R",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  14,
  number   =  5,
  pages    = "365--376",
  month    =  may,
  year     =  2013,
  url      = "http://www.nature.com/nrn/journal/v14/n5/full/nrn3475.html",
  language = "en",
  issn     = "1471-003X",
  doi      = "10.1038/nrn3475"
}


@ARTICLE{Poldrack2017-xc,
  title    = "Scanning the horizon: towards transparent and reproducible neuroimaging research",
  author   = "Poldrack, Russell A and Baker, Chris I and Durnez, Joke and Gorgolewski, Krzysztof J and Matthews, Paul M and Munaf{\`o}, Marcus R and Nichols, Thomas E and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal",
  journal  = "Nat. Rev. Neurosci.",
  volume   = "advance online publication",
  month    =  "5~" # jan,
  year     =  2017,
  url      = "http://www.nature.com/nrn/journal/vaop/ncurrent/full/nrn.2016.167.html",
  language = "en",
  issn     = "1471-003X",
  doi      = "10.1038/nrn.2016.167"
}

@article{GilmoreAdolph2017,
  author = {Gilmore, Rick O and Adolph, Karen E},
  title = "Video can make behavioural research more reproducible",
  journal = "Nature Human Behavior",
  volume = 1,
  year = 2017,
  month = June,
  day = 12,
  doi = {10.1038/s41562-017-0128}
}


@ARTICLE{Gilmore2017-qi,
  title       = "Progress toward openness, transparency, and reproducibility in
                 cognitive neuroscience",
  author      = "Gilmore, Rick O and Diaz, Michele T and Wyble, Brad A and
                 Yarkoni, Tal",
  affiliation = "Department of Psychology, the Pennsylvania State University,
                 University Park, Pennsylvania. Department of Psychology, the
                 Pennsylvania State University, University Park, Pennsylvania.
                 Social, Life, \& Engineering Sciences Imaging Center, the
                 Pennsylvania State University, University Park, Pennsylvania.
                 Department of Psychology, the Pennsylvania State University,
                 University Park, Pennsylvania. University of Texas at Austin,
                 Austin, Texas.",
  journal     = "Ann. N. Y. Acad. Sci.",
  publisher   = "osf.io",
  month       =  "2~" # may,
  year        =  2017,
  url         = "http://dx.doi.org/10.1111/nyas.13325",
  keywords    = "data sharing; open science; reproducibility",
  language    = "en",
  issn        = "0077-8923, 1749-6632",
  pmid        = "28464561",
  doi         = "10.1111/nyas.13325"
}

@ARTICLE{Bem2011-kl,
  title       = "Feeling the future: experimental evidence for anomalous
                 retroactive influences on cognition and affect",
  author      = "Bem, Daryl J",
  affiliation = "Department of Psychology, Uris Hall, Cornell University,
                 Ithaca, NY 14853, USA. d.bem@cornell.edu",
  journal     = "J. Pers. Soc. Psychol.",
  volume      =  100,
  number      =  3,
  pages       = "407--425",
  month       =  mar,
  year        =  2011,
  url         = "http://dx.doi.org/10.1037/a0021524",
  language    = "en",
  issn        = "0022-3514, 1939-1315",
  pmid        = "21280961",
  doi         = "10.1037/a0021524"
}


@ARTICLE{Wagenmakers2011-yh,
  title       = "Why psychologists must change the way they analyze their data:
                 the case of psi: comment on Bem (2011)",
  author      = "Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny
                 and van der Maas, Han L J",
  affiliation = "Department of Psychology, University of Amsterdam.
                 ej.wagenmakers@gmail.com",
  journal     = "J. Pers. Soc. Psychol.",
  volume      =  100,
  number      =  3,
  pages       = "426--432",
  month       =  mar,
  year        =  2011,
  url         = "http://dx.doi.org/10.1037/a0022790",
  language    = "en",
  issn        = "0022-3514, 1939-1315",
  pmid        = "21280965",
  doi         = "10.1037/a0022790"
}

@ARTICLE{Baker2017-da,
  title    = "Cancer reproducibility project releases first results",
  author   = "Baker, Monya and Dolgin, Elie",
  journal  = "Nature",
  volume   =  541,
  number   =  7637,
  pages    = "269--270",
  month    =  "18~" # jan,
  year     =  2017,
  url      = "http://dx.doi.org/10.1038/541269a",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "28102271",
  doi      = "10.1038/541269a"
}


@ARTICLE{Benjamin2017-am,
  title       = "Can cancer researchers accurately judge whether preclinical
                 reports will reproduce?",
  author      = "Benjamin, Daniel and Mandel, David R and Kimmelman, Jonathan",
  affiliation = "Biomedical Ethics Unit/STREAM, McGill University, Montreal,
                 Canada. York University, Department of Psychology, Toronto,
                 Canada. Biomedical Ethics Unit/STREAM, McGill University,
                 Montreal, Canada.",
  journal     = "PLoS Biol.",
  volume      =  15,
  number      =  6,
  pages       = "e2002212",
  month       =  jun,
  year        =  2017,
  url         = "http://dx.doi.org/10.1371/journal.pbio.2002212",
  language    = "en",
  issn        = "1544-9173, 1545-7885",
  pmid        = "28662052",
  doi         = "10.1371/journal.pbio.2002212"
}
